% references are stored in references.bib file
% to cite from it write '\cite{kour2014real,kour2014fast}'

% to paste a url:
% \begin{center}
%   \url{http://mirrors.ctan.org/macros/latex/contrib/natbib/natnotes.pdf}
% \end{center}

\documentclass{article}

\usepackage{arxiv}

%  Русский язык
\usepackage{multirow}
\usepackage{wrapfig}
\usepackage[T2A]{fontenc}			% кодировка
\usepackage[utf8]{inputenc}			% кодировка исходного текста
\usepackage[english,russian]{babel}	% локализация и переносы
\usepackage{hyperref}       % hyperlinks
\usepackage{url}    

% Математика
\usepackage{amsmath,amsfonts,amssymb,amsthm,mathtools} 

\usepackage{booktabs}       % professional-quality tables
\usepackage{amsfonts}       % blackboard math symbols
\usepackage{nicefrac}       % compact symbols for 1/2, etc.
\usepackage{microtype}      % microtypography
\usepackage{lipsum}
\usepackage{graphicx}
\usepackage{float}
\restylefloat{table}

\graphicspath{ {./images/} }



\usepackage{amsmath}


\title{Trends prediction}


\author{
  Zadvornov Egor \\
  MIPT\\
  \texttt{zadvornov.ev@phystech.edu} \\
 % examples of more authors
 %   \And
 % Zixuan Lu \\
 %  School of Coumputing and Information\\
 %  University of Pittsburgh\\
 %  Pittsburgh, PA 15213 \\
 %  \texttt{ZIL50@pitt.edu} \\
  %% \AND
  %% Coauthor \\
  %% Affiliation \\
  %% Address \\
  %% \texttt{email} \\
  %% \And
  %% Coauthor \\
  %% Affiliation \\
  %% Address \\
  %% \texttt{email} \\
  %% \And
  %% Coauthor \\
  %% Affiliation \\
  %% Address \\
  %% \texttt{email} \\
}

\begin{document}
\maketitle
\begin{abstract}
Analyzing and predicting trends in the media landscape is a complex task due to the volatility and instability of social trends and public interests. This study presents a novel approach that combines time series forecasting methods and topic modeling to tackle this challenge. The proposed framework leverages a multi-pronged clustering strategy, including embedding-based and topic modeling-based techniques, to identify thematic clusters within the media data. For each cluster, the study employs the state-of-the-art Prophet forecasting model to capture the unique characteristics and dynamics, enabling accurate predictions of future trends.

The results demonstrate the effectiveness of this hybrid approach, particularly in forecasting trends related to American football. However, the study also identifies limitations in predicting certain clusters that exhibit peculiarities, such as abrupt peaks and trend shifts. To address this, the study outlines future research directions, including the exploration of anomaly detection and forecasting algorithms specifically designed to handle complex time series patterns. By combining the strengths of time series forecasting and topic modeling, this work contributes to the advancement of trend prediction techniques in the dynamic and multifaceted media landscape, with potential applications in various domains beyond media, such as scientific publication trends and product demand forecasting.
\end{abstract}



\section{Introduction}
Analyzing and forecasting trends in the ever-evolving media landscape is a critical challenge with significant implications for various domains, including marketing, media production, public relations, and innovation research. The dynamic and volatile nature of social trends and public interests poses a formidable obstacle, as these phenomena often exhibit complex, non-linear, and rapidly changing patterns.

Our novel framework that integrates the time-series forecasting and topic modeling. Rather than relying on simplistic trend identification or manual curation, our framework employs a multi-pronged clustering strategy, including embedding-based and topic modeling-based techniques, to identify coherent thematic clusters within the media data. For each identified cluster, we then apply the state-of-the-art Prophet forecasting model, which has demonstrated exceptional capabilities in handling various time-series patterns, such as seasonality, trend changes, and outliers.

Currently, such models as ARIMA[link] or Exponential Smoothing[link] (e.g., Google Trends) are used to solve the task. These conventional approaches are applied directly to predict specific topics (trends) at the next point in time. The novelty of our method is the addition of a procedure for clustering topics and predicting the popularity of a whole class of trends. This approach seems to be more justified and useful, since it can be integrated into other algorithms, such as those for time series prediction on the predicted set of topics. One of the goals of this work is to check whether the algorithm we developed is superior to the methods described above. As demonstrated in Section “Forecasting”, the model is able to capture unique patterns in the data that simpler models, such as ARIMA or Exponential Smoothing, do not account for. Therefore, our framework demonstrates better metrics.

Furthermore, the framework can be extended to differentiate predictions based on social groups. This involves selecting a target social group (e.g., predicted cluster for sports), increasing the granularity of topics for that group (e.g., football, volleyball, Messi, etc.), and making predictions accordingly. This approach can provide more tailored and relevant insights for specific audience segments.

The potential impact of this work is vast, as the ability to reliably forecast media trends can have far-reaching implications across numerous industries and domains. From marketing and communication strategies to innovation research and product development, the insights gleaned from our framework can help organizations better anticipate and respond to the evolving interests and discussions of their target audiences.

Furthermore, the generalizability of our approach extends beyond the media domain, as the underlying principles can be applied to other complex and high-dimensional time-series data, such as trends in scientific publications, social movements, or consumer demand patterns. By pushing the boundaries of trend forecasting, this study aims to contribute to the advancement of time-series analysis and predictive modeling, with the ultimate goal of empowering decision-makers to navigate the dynamic and ever-changing landscapes of the modern world.









\section{Постановка задачи}

Пусть $T = \{T^k\}_{k=1}^K$ -- множество уникальных топиков, где $K$ -- общее число различных топиков. Имеется временной ряд $\{t_i\}_{i=1}^N$, соответствующий последовательности дат, и на каждую дату $t_i$ приходится набор $\{T_{ij}\}_{j=1}^{M_i}$ популярных топиков, где $M_i$ -- число популярных топиков в день $t_i$. Каждый $T_{ij} \in T$ представляет собой одно слово или фразу длиной до 4 слов.

Цель данной работы -- предсказать будущую популярность топиков. Для достижения этой цели предлагается следующий алгоритм:

\subsection{Кластеризация топиков}

1. Выполняется кластеризация множества $T$ на $n$ семантических кластеров $c_m = \{T^k\}_{k=1}^{n_m}$, $m = 1, \ldots, n$, где $n_m$ -- число топиков в кластере $c_m$. Для определения оптимального числа кластеров $n$ используется средняя мера когерентности $C_{\text{cv}}$, которая оценивает интерпретируемость кластеров человеком путем измерения семантической близости между словами внутри кластера:

\begin{equation}
n = \arg\min_{L \in \mathbb{N}} \frac{1}{L} \sum_{m=0,...,L-1} C_{\text{cv}}(c_m)
\end{equation}


Мера $C_{\text{cv}}$ рассчитывается следующим образом:

\begin{equation}
C_{\text{cv}} = \frac{1}{|S^{\text{one}}_{\text{set}}|} \sum_{(W_0, W_*) \in S^{\text{one}}_{\text{set}}} \tilde{m}_{\cos(\text{nlr}, 1)}(W_0, W_*)
\end{equation}

где $S^{\text{one}}_{\text{set}} = \{(W_0, W_*) | W_0 = \{w_i\}, w_i \in W, W_* = W\}$ -- сегментация, представляющая множество пар подмножеств слов, $\tilde{m}_{\cos(\text{nlr}, 1)}(W_0, W_*)$ -- косинусное сходство между векторами контекста $W_0$ и $W_*$, оценивающее, насколько хорошо $W_*$ "поддерживает" или "объясняет" $W_0$.

2. Для каждого кластера $c_m$ строится временной ряд $\{t_i, y_m^i\}_{i=1}^N$, где $y_m^i$ -- число появлений топиков из кластера $c_m$ в день $t_i$.

\subsection{Прогнозирование популярности топиков}

Популярные модели обработки временных рядов, такие как Prophet, ARIMA и Exponential Smoothing, обучаются на тренировочных наборах данных $\{t_i, y_m^i\}_{train}$, $0 \leq i < N_{train}$, для каждого кластера $c_m$. Обученные модели затем применяются на тестовой выборке $\{t_i, y_m^i\}_{test}$, $N_{train} \leq i < N_{train} + N_{test}$, для предсказания числа появлений $y_m^i$ топиков $\{T^k\}_{k=1}^{n_m}$ из кластера $c_m$ в будущие моменты времени $t_i$.

\subsubsection{Модель Prophet}

Модель Prophet представляет собой аддитивную регрессионную модель, декомпозирующую временной ряд на следующие компоненты:

\begin{equation}
y(t) = g(t) + s(t) + h(t) + \epsilon_t
\end{equation}

где $g(t)$ -- компонента тренда, $s(t)$ -- компонента сезонности, $h(t)$ -- компонента, отражающая эффекты праздников, $\epsilon_t$ -- случайная ошибка.

Компонента тренда $g(t)$ может быть представлена в виде логистического роста $g(t)_{log}$ или кусочно-линейного тренда с точками изменения $g(t)_{lin}$ :

\begin{align}
g(t)_{lin} &= (k + \sum_{j=1}^S \delta_j)t + (m + \sum_{j=1}^S \gamma_j) &&\text{(линейный тренд)} \\
g(t)_{log} &= \frac{C(t)}{1 + \exp{(-g(t)_{lin})}} &&\text{(нелинейный логистический рост)}
\end{align}

Сезонная компонента $s(t)$ моделируется с помощью ряда Фурье:

\begin{equation}
s(t) = \sum_{n=1}^N \left(a_n \cos\left(\frac{2\pi n t}{P}\right) + b_n \sin\left(\frac{2\pi n t}{P}\right)\right)
\end{equation}

Эффекты праздников $h(t)$ моделируются как сумма индикаторных функций для каждого праздничного дня $i$:

\begin{equation}
h(t) = \sum_{i=1}^L \theta_i \mathbb{1}_{t \in D_i}
\end{equation}

Оценивание и подбор оптимальных параметров модели $\hat{\theta}$ выполняется с использованием метода максимального правдоподобия:

\begin{equation}
\hat{\theta} = \arg\min_{\theta} -\log p(y | \theta) \left\{ = \frac{n}{2}\log(2\pi) + \frac{n}{2}\log(\sigma^2) + \frac{1}{2\sigma^2}\sum_{t} \left(y(t) - \sum_{i} f_i(t)\right)^2 \right\}
\end{equation}

где $\theta$ -- параметры модели, $n$ -- количество наблюдений, $\sigma^2$ -- дисперсия ошибок, $y(t)$ -- фактические значения, $f_i(t)$ -- компоненты модели.

Прогнозирование осуществляется путем экстраполяции тренда, использования сезонной компоненты и эффектов праздников:

\begin{equation}
\hat{y}(t+h) = \hat{g}(t+h) + \hat{s}(t+h) + \hat{h}(t+h)
\end{equation}

\subsubsection{Модель SARIMA}

Модель SARIMA (Seasonal Autoregressive Integrated Moving Average) является расширением ARIMA для учета сезонности во временных рядах. Модель $SARIMA(p,d,q)(P,D,Q)_m$ может быть представлена следующим образом:

\begin{equation}
(1-\phi_1 B) (1-\Phi_1 B^m)(1-B)^d(1-B^m)^D y_t = (1+\theta_1 B) (1+\Theta_1 B^m)\epsilon_t
\end{equation}

где $p$ -- порядок несезонной авторегрессионной части, $d$ -- степень несезонного дифференцирования, $q$ -- порядок несезонной части скользящего среднего, $P$ -- порядок сезонной авторегрессионной части, $D$ -- степень сезонного дифференцирования, $Q$ -- порядок сезонной части скользящего среднего, $m$ -- число периодов в сезонном цикле, $\phi_1$ -- параметр несезонной авторегрессионной части, $\Phi_1$ -- параметр сезонной авторегрессионной части, $\theta_1$ -- параметр несезонной части скользящего среднего, $\Theta_1$ -- параметр сезонной части скользящего среднего, $B$ -- оператор сдвига назад ($B^j y_t = y_{t-j}$), $\epsilon_t$ -- белый шум.

Алгоритм модели SARIMA включает следующие шаги:

1. Идентификация модели: определение значений $p$, $d$, $q$, $P$, $D$, $Q$ и $m$ на основе анализа автокорреляционной (ACF) и частной автокорреляционной функций (PACF), а также тестов на стационарность.

2. Оценка параметров: оценка параметров $\phi_1$, $\Phi_1$, $\theta_1$, $\Theta_1$ и дисперсии шума $\sigma^2$ методом максимального правдоподобия или условным методом наименьших квадратов.

3. Диагностика модели: проверка адекватности модели путем анализа остатков на наличие автокорреляции, нормальности и гомоскедастичности.

4. Прогнозирование: получение точечных и интервальных прогнозов на основе оцененной модели:

\begin{equation}
\hat{y}_{t+h|t} = \phi_1 \hat{y}_{t+h-1|t} + \Phi_1 \hat{y}_{t+h-m|t} - \theta_1 \hat{\epsilon}_{t+h-1|t} - \Theta_1 \hat{\epsilon}_{t+h-m|t}
\end{equation}

где $\hat{\epsilon}_{t+h|t} = 0$ для $h > 0$.

\subsubsection{Метод Хольта-Винтерса (Exponential Smoothing)}

Метод Хольта-Винтерса является расширением модели экспоненциального сглаживания для учета тренда и сезонности. Аддитивный метод Хольта-Винтерса представлен следующим образом:

\begin{equation}
\hat{y}_{t+h|t} = l_t + h b_t + s_{t+h-m(k+1)}
\end{equation}

где $\hat{y}_{t+h|t}$ -- прогноз на $h$ шагов вперед, сделанный в момент t, $l_t$ -- уровень ряда, $b_t$ -- тренд, $s_t$ -- сезонная компонента, $m$ -- число периодов в сезонном цикле, $k = \lfloor(h-1)/m\rfloor$.

Алгоритм метода Хольта-Винтерса:

1. Инициализация начальных значений для уровня $l_0$, тренда $b_0$ и сезонных компонент $s_0$, $s_{-1}$, $\ldots$, $s_{-(m-1)}$.

2. На каждом шаге $t$ обновляются уровень $l_t$, тренд $b_t$ и сезонная компонента $s_t$ по формулам:

\begin{align}
l_t = \alpha(y_t - s_{t-m}) + (1-\alpha)(l_{t-1} + b_{t-1})\
\end{align}
\begin{align}
b_t = \beta(l_t - l_{t-1}) + (1-\beta)b_{t-1}\
\end{align}
\begin{align}
s_t = \gamma(y_t - l_t) + (1-\gamma)s_{t-m}
\end{align}

где $\alpha$, $\beta$, $\gamma$ -- сглаживающие параметры.

3. Прогнозирование с использованием обновленных оценок уровня, тренда и сезонности:
\begin{equation}
\hat{y}_{t+h|t} = l_t + hb_t + s_{t+h-m(k+1)}
\end{equation}

4. Оценка параметров $\alpha$, $\beta$, $\gamma$ путем минимизации суммы квадратов ошибок прогноза.


\subsection{Критерии качества прогноза}

Обозначим через  $\mathbf{Y}_m \in R^{N_{test}}$ упорядоченный набор из  фактических значений временного ряда  кластера $c_m$ в моменты $t_i$, $N_{train} \leq i < N_{train} + N_{test}$ :
\begin{equation}
     \mathbf{Y}_m = [y_m^{N_{train}}, \ldots, y_m^{N_{train} + N_{test} - 1}]^T.
\end{equation}

Получим
$\{\mathbf{Y}_m\}_{m=1}^{n}$

 
%  \begin{equation}
%     \mathbf{Y} = [\mathbf{Y}_1, \ldots, \mathbf{Y}_n]^T.
% \end{equation}. 

В качестве критериев качества прогноза временных рядов используются среднее абсолютное отклонение (MAE) и средний квадрат ошибки (MSE):

\begin{equation}
\text{MAE} = \frac{1}{n}\sum\limits_{i = 1}^{n}|\mathbf{Y}_i - \mathbf{\hat{Y}}_i|
\end{equation}

\begin{equation}
\text{MSE} = \frac{1}{n}\sum\limits_{i = 1}^{n}(\mathbf{Y}_i - \mathbf{\hat{Y}}_i)^2
\end{equation}

где $\{\mathbf{\hat{Y}}_m\}_{m=1}^{n}$ -- прогнозные значения $n$ -- число временных рядов.





\subsection{Задачи исследования}

Основными задачами данной работы являются:

Выбор наилучшего алгоритма кластеризации топиков из двух вариантов: Embeddings + K-Means и LDA.

Сравнение популярных моделей для прогнозирования временных рядов, а именно ARIMA, Prophet и Exponential Smoothing, и проверка гипотезы о том, что модель Prophet дает наилучшие значения метрики MSE.

Сравнение метрик наилучшей модели для прогнозирования в случае предсказания числа вхождений $y_i$ каждого отдельного топика $T^k$ и в случае предсказания числа $y_m^i$ внутри кластера $c_m$. Проверка гипотезы о том, что подход с кластеризацией топиков будет давать лучшие мет







% ----------------------- OLD--------------------------------------


\section{Data construction}
\label{sec:headings}
The dataset used in this study consists of publicly available posts on social media platforms, such as Twitter, and the most popular search queries through browsers, such as Google, over several years. The data was collected from media aggregation platforms and spans the period from January 1, 2019, to March 3, 2024.

The dataset includes the top 15 news topics from both Twitter and Google for each day during this time period, resulting in a total of 76,140 observations. The dataset has the following structure:

\begin{table}[h]
\centering
\begin{tabular}{|c|c|c|}
\hline
\textbf{Time} & \textbf{Source} & \textbf{Topic}           \\ \hline
2024-03-03    & Twitter         & Rashford                 \\ \hline
2024-03-03    & Twitter         & \#sundayvibes            \\ \hline
2024-03-03    & Twitter         & Xavier Worthy            \\ \hline
2024-03-03    & Twitter         & Foden                    \\ \hline
2024-03-03    & Twitter         & \#UFCVegas87             \\ \hline
...           & ...             & ...                      \\ \hline
2017-03-18    & Google          & Robert Osborne           \\ \hline
2017-03-18    & Google          & Alejandra Campoverdi     \\ \hline
2017-03-18    & Google          & Drake More Life          \\ \hline
2017-03-18    & Google          & Drake More Life Download \\ \hline
2017-03-18    & Google          & Costco Travel            \\ \hline
\end{tabular}
\caption{Sample of data}
\label{tab:timefromtime}
\end{table}

The dataset includes information on the timestamp, source (Twitter or Google), and the specific topic or search query.
The target variable (Y) in this dataset is the Topic, representing the popular topics discussed on social media and search engines over time. The predictor variable (X) is the Time, which represents the date and serves as the input for forecasting the future trends in the media landscape.
Prediction is feasible in this dataset due to the presence of cyclic/periodic patterns in the target variable (Y) over time (X). The topics discussed in the media often exhibit seasonal and temporal patterns, which can be leveraged to forecast future trends.
To assess the generalizability of the proposed approach, the study also tested the model on two additional datasets:
Twitter Trending Tweets [5]: This dataset contains information on the daily trending tweets on Twitter, including the topic and its significance.
YouTube Trending Video Dataset [6]: This dataset includes data on the daily trending YouTube videos, such as the video title, channel, and various engagement metrics.
However, the results from these additional datasets were not as promising as the primary dataset, and the details are provided in the Appendix.
The primary dataset used in this study offers a comprehensive representation of the media landscape, covering both social media and search engine trends. The combination of Twitter and Google data provides a well-rounded view of the evolving public interests and discussions, making it a suitable testbed for the proposed trend forecasting framework.
We applied the model to this dataset, see this description in the next paragraphs.


\section{Algorithms and Models}

\subsection{Pipeline of the research}

The proposed approach aims to tackle the challenge of trend prediction in the media landscape. The scheme of the pipeline is presented at ~\ref{fig:pipeline}.

\begin{figure}[h] 
    \centering
    \includegraphics[width=15cm,height=6cm,keepaspectratio]{pipeline.png}
    \label{fig:pipeline}
    \caption{Image of the pipeline structure.}
\end{figure}

%TODO: rearrange in lists
The key steps of the pipeline are:

Preprocessing and Clustering:
    The raw media data (e.g., social media posts, search trends) is preprocessed to remove noise and extract relevant features.
    Reduce domain volatility through week-by-week binarisation
    A multi-pronged clustering strategy is employed to identify coherent thematic clusters within the media data. This includes techniques such as:
        - Embedding-based clustering using sentence embeddings and K-Means
        - Topic modeling using Latent Dirichlet Allocation (LDA) on the topic representations
        
Significance Estimation:
    For each identified thematic cluster, the significance of the events or topics is estimated based on their position in the media landscape (e.g., ranking in trending topics).
    This significance estimation helps to prioritize the most relevant and impactful trends for the subsequent forecasting stage.
    
Time Series Forecasting:
    The time series of each thematic cluster is modeled using the state-of-the-art Prophet forecasting algorithm.
    The Prophet model is chosen for its ability to handle various time series patterns, such as seasonality, trend changes, and outliers.
    By applying the Prophet model to the individual thematic clusters, the framework can capture the unique characteristics and dynamics of each topic, leading to more accurate and nuanced predictions.
    
Ensemble Forecasting:
    The forecasts from the individual thematic clusters are combined to provide a comprehensive prediction of the future media landscape.
    
Anomaly Detection and Forecasting:
    The study identifies the need to explore existing algorithms specifically designed for anomaly detection and forecasting in time series data.
    These advanced techniques hold the potential to enhance the predictive capabilities of the proposed framework across a wider range of topic clusters, particularly those exhibiting complex or irregular patterns.


\subsection{Clustering}

\subsubsection{Theory}
We employed a multi-pronged approach to clustering the time series data, experimenting with several techniques to identify the most effective method. Here are the top 2 of them

\begin{itemize}
\item Embeddings + K-Means
\item Topic Modeling on Topics using Latent Dirichlet Allocation (LDA)
\end{itemize}

Key steps of the first approach are Embedding Generation, which utilized the Sentence Transformers library [1] to generate contextual embeddings for each topic using the all-MiniLM-L12-v2 model [2], and then K-Means Clustering applied to embedded topics. Since both the embedding procedure and the k-means algorithm are widely known operations, we do not provide a detailed description of them in this work, leaving only a reference link.

The second algorithm works as follows. The preprocessing steps included lemmatization, stop word removal, and keeping words of length 2-3. Then the LDA model is created. [add description of how the LDA model works].
The aim of the section is to compare described approaches by clustering quality and choose one to use in the Forecasting section.
The next criteria for selecting the best algorithm of these two, based on the coherence[add link!] metric, was elaborated: 
<rewrite as formula in latex with proper definitions>
If | coherence embed - coherence LDA | < 0.05 AND coherence embed > 0.5
then: “We choose embedding”
else: “We choose LDA”
Thus, we allow a slight loss of the k-means algorithm to the LDA in metric value, since it is the more simple model and outperforms LDA.



The metric coherence was chosen since it … <why it is a proper choice>. The algorithm of coherence calculation is described at [add link!]. The main steps are <some summary of algorithm>.

% ``` Добавлю
% Как считаются все возможные coherence описано в этой статье (https://svn.aksw.org/papers/2015/WSDM_Topic_Evaluation/public.pdf)
% В нашем случае 'c_v' описана в таблице 2 и основывается на формуле 23, если я правильно понял
% ```

The next two subsections outline the main ideas of the compared algorithms and their clustering results.

\subsubsection{Embeddings + K-Means}

The Embeddings + K-Means approach was used to perform the initial clustering of the media topics. The key steps are as follows:
Embedding Generation: We utilized the Sentence Transformers library [1] to generate contextual embeddings for each topic using the all-MiniLM-L12-v2 model [2]. These embeddings capture the semantic similarity between the topics.
K-Means Clustering: The generated embeddings were then fed into the K-Means clustering algorithm to group the topics based on their semantic similarity.
To determine the optimal number of clusters (K), we evaluated several metrics. The mean coherence score, which quantifies the interpretability of clusters to humans by measuring the semantic similarity among the top words within a cluster. The optimal number of clusters was found to be 16, as shown in the figure:

\begin{figure}[h] % picture
    \centering
    \includegraphics[width=15cm,height=6cm,keepaspectratio]{MeanCohKMeans.png}
    \label{fig:MeanCohKMeans}
    \caption{Coherence score vs. number of clusters for the Embeddings + K-Means algorithm.}
\end{figure}

The resulting clustering with expert interpretation is presented at Table 2. The mean coherence score for the 16 clusters was 0.63.

\begin{table}[h]
\centering
\begin{tabular}{|l|l|l|}
\hline
\multicolumn{1}{|c|}{\textbf{Cluster \#}} & \multicolumn{1}{c|}{\textbf{Interpretation}}                    & \multicolumn{1}{c|}{\textbf{Coherence}} \\ \hline
0                                         & NFL                                                             & 0.63                                    \\ \hline
1                                         & Football                                                        & 0.43                                    \\ \hline
2                                         & Motivational Hashtags                                           & 0.75                                    \\ \hline
3                                         & Sports Personalities                                            & 0.58                                    \\ \hline
4                                         & Entertainment and Celebrities                                   & 0.58                                    \\ \hline
5                                         & Music                                                           & 0.55                                    \\ \hline
6                                         & Politics and Holidays                                           & 0.73                                    \\ \hline
7                                         & UFC                                                             & 0.74                                    \\ \hline
8                                         & Emotional Hashtags                                              & 0.70                                    \\ \hline
9                                         & Social events Hashtags                                          & 0.73                                    \\ \hline
10                                        & Political Figures and Events                                    & 0.59                                    \\ \hline
11                                        & Basketball                                                      & 0.37                                    \\ \hline
12                                        & Celebrities and Personalities                                   & 0.65                                    \\ \hline
13                                        & Business                                                        & 0.65                                    \\ \hline
14                                        & World Events and Countries                                      & 0.71                                    \\ \hline
15                                        & Entertainment                                                   & 0.68                                    \\ \hline
Mean                                      & -                                                               & 0.63                                    \\ \hline
\end{tabular}
\caption{Embed+Kmeans clustering result}
\label{tab:KmeansClusters}
\end{table}

The Embeddings + K-Means approach demonstrated good thematic coherence within the clusters, allowing for meaningful interpretation and subsequent forecasting. The clusters captured distinct themes, such as business, sports, politics, and entertainment, providing a solid foundation for the time series forecasting component of the study.
Examples, references to the algorithms, clustering images and other details of the Embeddings + K-Means algorithms are given in the appendix.

\subsubsection{Latent Dirichlet Allocation (LDA) on Topics}

To address the limitations of the embedding-based approach, we explored topic modeling using Latent Dirichlet Allocation (LDA) [link!] directly on the topic representations. This allowed us to capture the latent thematic structures within the data.
We implement the algorithm as follows. The preprocessing steps included lemmatization, stop word removal, and keeping words of length 2-3. Then the LDA model is created. [add description how the LDA model works].
The coherence score increased with a higher number of clusters, as shown in the Figure 3.

\begin{figure}[h] 
    \centering
    \includegraphics[width=15cm,height=6cm,keepaspectratio]{MeanCohKMeans.png}
    \label{fig:MeanCohKMeans}
    \caption{Mean coherence score vs number of clusters}
\end{figure}

According to the plot there is no dependence of LDA on the number of clusters. Therefore, to provide a more accurate comparison, we set 16 clusters and calculated the Coherence metrics (Table 3). 

\begin{table}[h]
\centering
\begin{tabular}{|l|l|l|}
\hline
\multicolumn{1}{|c|}{\textbf{Cluster \#}} & \multicolumn{1}{c|}{\textbf{Interpretation}} & \multicolumn{1}{c|}{\textbf{Coherence}} \\ \hline
0                                         &                                              & 0.69                                    \\ \hline
1                                         &                                              & 0.71                                    \\ \hline
2                                         &                                              & 0.68                                    \\ \hline
3                                         &                                              & 0.67                                    \\ \hline
4                                         &                                              & 0.70                                    \\ \hline
5                                         &                                              & 0.71                                    \\ \hline
6                                         &                                              & 0.67                                    \\ \hline
7                                         &                                              & 0.71                                    \\ \hline
8                                         &                                              & 0.69                                    \\ \hline
9                                         &                                              & 0.68                                    \\ \hline
10                                        &                                              & 0.68                                    \\ \hline
11                                        &                                              & 0.72                                    \\ \hline
12                                        &                                              & 0.71                                    \\ \hline
13                                        &                                              & 0.71                                    \\ \hline
14                                        &                                              & 0.69                                    \\ \hline
15                                        &                                              & 0.67                                    \\ \hline
Mean                                      & -                                            & 0.69                                    \\ \hline
\end{tabular}
\caption{LDA clustering result}
\label{tab:LDAClusters}
\end{table}

\subsubsection{Summary}
According to criteria in the beginning of the Theory part, the algorithm “Embedding + K means” was chosen. The pivot table of metrics for the selected models are as follows:

\begin{table}[h]
\centering
\begin{tabular}{|l|l|l|}
\hline
\textbf{Model} & \textbf{Mean Coherence} & \multicolumn{1}{c|}{\textbf{Coherence}} \\ \hline
K-means        & 0.63 +- ?               & 0.69                                    \\ \hline
LDA            & 0.693 +- 0.004          & 0.71                                    \\ \hline
\end{tabular}
\caption{Resulted metrics of each algorithm. The error calculated by “error of the mean” formula.
}
\label{tab:ClusteringPivotTable}
\end{table}

The LDA algorithm is better according to the Mean Coherence metric. Furthermore, it has more stable per-cluster Coherence than Embeddings + K-Means algorithm. Therefore, we have chosen to utilize LDA  for further analysis. In the next section, we apply the forecast model to clusters-outcomes after LDA clustering.

\subsection{Forecasting}

\subsubsection{Theory}

In this Section different architectures are applied to forecast popularity of topics. The input of the models is a week-by-week time series of clustered social media topics X, the prediction is a set of integer numbers $N_i$ – the counts of posts, containing the cluster i in the next timestep (week).

%<Дописать, что по сути создаются N разных моделей (N разных наборов гиперпараметров) под предсказание числа постов каждого кластера в отдельности>
Three models were investigated during the research. The first one is the Prophet, developed by researchers at Facebook's Core Data Science team. It is a decomposable time series model that combines several fundamental components to capture the dynamics of a time series [4]. The key advantages of the Prophet model include its ability to handle various types of time series patterns, such as seasonality, trend changes, and outliers, as well as its scalability and ease of use. Basic steps of the algorithm are [add description from habr post]. For a more detailed description one can read [link! from habr post].
The second model called ARIMA works like [add description with links]. 
The last model we experimented with is Exponential Smoothing. The algorithm of its operation is [add description with links].
Since all the models were evaluated on the same dataset, the MAE and MSE metrics were chosen to compare their quality.

\subsubsection{Prophet model}

The forecasting component of the proposed framework represents a crucial step in predicting future trends in the media landscape. To address the challenge of accurately forecasting complex and volatile time series data, the study employed a state-of-the-art forecasting model, the Prophet algorithm [3].
After applying the Embedding algorithm, we conducted forecasts for each result. The following table presents the final metrics for the forecasts:

\begin{table}[h]
\centering
\begin{tabular}{|l|l|l|}
\hline
\textbf{Clusters Count} & \textbf{Average MAE} & \textbf{Average MSE} \\ \hline
5                       & 0.22                 & 0.08                 \\ \hline
7                       & 0.23                 & 0.08                 \\ \hline
9                       & 0.25                 & 0.10                 \\ \hline
16                      & 0.28                 & 0.13                 \\ \hline
\end{tabular}
\caption{Resulted Prophet metrics.}
\label{tab:ProphetMetrics}
\end{table}

For the American football cluster and Political cluster, the Prophet model was able to accurately capture the seasonality and life cycle of the topic, as shown in the following figures:

\begin{figure}[h] 
    \centering
    \includegraphics[width=15cm,height=6cm,keepaspectratio]{ProphetPreds.png}
    \label{fig:ProphetPreds}
    \caption{Prophet forecast for American football cluster.}
\end{figure}

% -----------------------------------------------------------------------------------





\bibliographystyle{unsrt}  
\bibliography{references} 
\bibitem{Taylor2018}
Taylor, S. J. and Letham, B. (2018). Forecasting at scale. {\em The American Statistician}, 72(1):37--45. doi:10.1080/00031305.2017.1380080. https://peerj.com/preprints/3190/
\bibitem{Reimers2019}
Reimers, N. and Gurevych, I. (2019). Sentence-BERT: Sentence Embeddings using Siamese BERT-Networks. {\em arXiv preprint arXiv:1908.10084}. https://arxiv.org/abs/1908.10084
\bibitem{Motrenko2014}
Motrenko, A. and Strijov, V. (2014). Extracting fundamental periods to segment biomedical signals. https://m1p.org/papers/MotrenkoStrijov2014RV2.pdf
\bibitem{Das2020VAE}
Das, A. (2020). Foundation of Variational Autoencoder (VAE). https://ayandas.me/blog-tut/2020/01/01/variational-autoencoder.html
\bibitem{Grabovoy2019}
Grabovoy, A. V. and Strijov, V. V. (2019). Quasi-periodic time series clustering for human activity recognition. https://m1p.org/papers/Grabovoy2019QuasiPeriodicTimeSeries.pdf
\bibitem{Uvarov2018}
Uvarov, N. D., Kuznetsov, M. P., Malkova, A. S., Rudakov, K. V., and Strijov, V. V. (2018). Selection of superposition of models for railway freight forecasting. https://m1p.org/papers/Uvarov2018SuperpositionForecasting_eng.pdf
\bibitem{Shumway2000}
Shumway, R. H., Stoffer, D. S., and Stoffer, D. S. (2000). {\em Time series analysis and its applications}, volume 3. Springer.
\bibitem{Gardner1985}
Gardner, E. S. (1985). Exponential smoothing: The state of the art. {\em Journal of Forecasting}.
\bibitem{Bouchaud2018}
Bouchaud, J.-P., Bonart, J., Donier, J., and Gould, M. (2018). {\em Trades, Quotes and Prices, Financial Markets Under the Microscope}. Cambridge University Press. https://www.cambridge.org/core/books/trades-quotes-and-prices/029A71078EE4C41C0D5D4574211AB1B5
\bibitem{Garleanu2013}
Gˆarleanu, N. and Pedersen, L. H. (2013). Dynamic Trading with Predictable Returns and Transaction Costs. https://papers.ssrn.com/sol3/papers.cfm?abstract_id=1364170
\bibitem{TwitterTrends}
Twitter Trending Tweets. https://www.kaggle.com/datasets/rsrishav/twitter-trending-tweets
\bibitem{YouTubeTrends}
YouTube Trending Video Dataset. https://www.kaggle.com/datasets/rsrishav/youtube-trending-video-dataset
\bibitem{SentenceTransformers}
Sentence Transformers: all-MiniLM-L12-v2 t. https://huggingface.co/sentence-transformers/all-MiniLM-L12-v2


%%% Remove comment to use the external .bib file (using bibtex).
%%% and comment out the ``thebibliography'' section.


% %%% Comment out this section when you \bibliography{references} is enabled.
% \begin{thebibliography}{1}

% \bibitem{kour2014real}
% George Kour and Raid Saabne.
% \newblock Real-time segmentation of on-line handwritten arabic script.
% \newblock In {\em Frontiers in Handwriting Recognition (ICFHR), 2014 14th
%   International Conference on}, pages 417--422. IEEE, 2014.

% \bibitem{kour2014fast}
% George Kour and Raid Saabne.
% \newblock Fast classification of handwritten on-line arabic characters.
% \newblock In {\em Soft Computing and Pattern Recognition (SoCPaR), 2014 6th
%   International Conference of}, pages 312--318. IEEE, 2014.

% \bibitem{hadash2018estimate}
% Guy Hadash, Einat Kermany, Boaz Carmeli, Ofer Lavi, George Kour, and Alon
%   Jacovi.
% \newblock Estimate and replace: A novel approach to integrating deep neural
%   networks with existing applications.
% \newblock {\em arXiv preprint arXiv:1804.09028}, 2018.
%\end{thebibliography}






\end{document}
